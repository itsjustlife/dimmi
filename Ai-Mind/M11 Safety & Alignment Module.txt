Module M11: Safety & Alignment Module

---
Module: M11_Safety_Alignment
Version: 1.0
Purpose: Enforce safety policies and alignment with user instructions throughout the AI's operations
Triggers: [ "Content_Check_Request", "Action_Check_Request", "Output_Filter_Request" ]
Inputs: [ "Proposed_Action or Output", "Contextual_Info" ]
Outputs: [ "Safety_Approval", "Filtered_Output" ]
Dependencies: [ ]
Safety_Level: High
Checksum: 650ff88cd15e8c5df805b34d21b5f349
---

PURPOSE:
 The Safety & Alignment Module is the AI’s conscience and gatekeeper. Its job is to scrutinize the AI’s intended actions and communications to ensure they are compliant with predefined safety rules, ethical guidelines, and the user’s explicit instructions. It works at multiple stages:
 - Before an action is executed by M10, M11 reviews it (especially if it might affect external systems or sensitive data).
 - Before anything is said to the user via M14, M11 filters the content to remove or alter any disallowed information (e.g., personal data, harmful advice, etc.).
 - It also ensures alignment: that the AI’s responses stay within the user’s goals (avoiding going on tangents or doing something user didn’t ask for) and the system’s policies (like not giving medical advice, respecting privacy, etc.).
In human analogy, this is like the superego or an oversight committee that prevents impulsive or rule-breaking behavior.

INPUTS:
 - **Proposed_Action or Output:** This can be:
   - An action description from M09/M10 that is about to be executed (e.g., "Delete file X", "Call API Y", "Search query Z").
   - A response or message that M14 is about to present to the user.
   - Essentially anything that might need vetting. The triggers differentiate the use case (Content vs Action vs Output filter, though content vs output might overlap; content check and output filter both deal with text to user, likely).
 - **Contextual_Info:** Additional info to help in decision:
   - For actions: maybe the goal or reasoning behind it (to decide if it’s justified).
   - For content: the conversation context or user’s last message (to understand if something is relevant or how the user phrased things; also to avoid filtering something that was actually user-provided unless needed).
   - Could include safety configuration (like user’s age if relevant to restrict content, or specific mode like "safe mode on/off" if that existed).

OUTPUTS:
 - **Safety_Approval:** A signal or decision regarding an action:
   - e.g., "approve", "reject", or "modify".
   - If reject, possibly with reason code (like "policy_violated: self_harm_advice").
   - If modify, perhaps instructions on how (like "remove part about X and it’s fine").
   - M09/M10 would then act accordingly (not execute if not approved, etc.).
 - **Filtered_Output:** In case of user-facing content, the sanitized version of the text:
   - E.g., if the AI’s raw answer had a disallowed sentence, the filtered output would be without it or replaced with "[removed]".
   - If everything is fine, it might be identical to input.
   - If nothing can salvage it (the whole message is disallowed), filtered output might be an apology or refusal message, or blank – depending on policy (maybe M11 or M14 would then decide to produce a safe fallback).
   - The transformation should aim to maintain as much useful content as possible while removing only what's necessary (to avoid unnecessary censorship but still be safe).
 - Note: Often Safety modules are implemented as a set of rules or even another model. In our case, think of it as rule-based system, possibly with some lists of forbidden topics etc., since we can't incorporate a full ML moderation model here.

DETAILED_DECISION_LOGIC:
 1. **Check Proposed Action** (on Action_Check_Request):
    - Evaluate the nature of the action:
      - Is it attempting to access or modify something restricted? (e.g., writing to system files, or calling an API outside allowed scope)
      - Does it involve interacting with a user or third-party in a way that could be sensitive? (e.g., sending an email to someone might have privacy concerns).
      - Use a policy list: e.g., "disallowed actions = [access user personal files, network calls to untrusted domains, executing OS commands, etc.]"
      - If the action appears on the disallowed list or matches a pattern, output **Safety_Approval = reject** with reason.
      - If the action is borderline (not outright disallowed but potentially harmful if used improperly), maybe flag it: output modify or require confirmation. However, our system probably doesn’t do user confirmation unless specifically implemented. More likely we either allow or disallow. We could do "caution" but M09 might just treat that as rejection unless there's a mechanism to override.
      - If action is clearly safe and within scope, output approve.
    - Possibly adjust action: For example, if the action is "search: unfiltered content", safety might say "search: safe mode" if that concept exists. That would be a modify scenario where we tweak parameters to be safer. Output instructions to M10 if needed (like adding a safe search parameter).
 2. **Check Content for User** (on Content_Check_Request/Output_Filter_Request):
    - Analyze the text:
      - Check for profanity, harassment, hate speech, etc. Remove or mask those.
      - Check for privacy: If the text contains sensitive personal data (maybe from memory or user data that shouldn't be repeated without permission), either remove it or generalize it. E.g., don't output someone's full name or address if not allowed.
      - Check for disallowed topics: e.g., advice on illicit activities, self-harm. If found:
        * Either remove just that part and see if the rest of answer still stands.
        * Or if the entire answer is about that forbidden topic (like user asked for it and got it), then we have to refuse. In that case, filtered output might be replaced with a safe refusal message like "I'm sorry, I cannot assist with that request." The policy might define a standard refusal statement.
      - Check alignment: Ensure the answer is actually addressing the user’s query and not going off-track or inserting unsolicited content. This is harder to quantify, but for example if the answer included a random political opinion unrelated to user’s request, alignment filter might remove it or flag it.
      - Also ensure factual safety if needed: e.g., if giving medical info, maybe ensure a disclaimer or that the model isn't confident-sounding if not sure. This might be out of scope for a simple implementation, but mention it if a policy exists like "for medical, always include: I'm not a doctor".
    - Construct **Filtered_Output**:
      - If minor edits: implement them (e.g., replace bad words with ****, or cut out a sentence).
      - If major issue: decide refusal. Possibly M11 can directly output a refusal message. Or M11 can signal to M14 that content was disallowed entirely, and M14 then decides to use a fallback response. But as an output, probably easier if M11 itself provides the safe response. We'll assume it can.
      - Ensure the final output still is coherent. If removal left awkward gap, maybe insert an ellipsis or brief note if needed. Usually, we try not to disrupt flow unless necessary.
      - Mark the approval accordingly: maybe Safety_Approval in this context is implicit in providing filtered content (if filtered content is significantly different, that implies original was not fully safe).
 3. **Feedback & Learning**:
    - Optionally, log any safety interventions. For instance, if an action was rejected or content filtered, maybe send a note to reflection (M12) or to long-term memory log that "We had to refuse this due to policy X". This can help the AI learn or the developers to refine the system.
    - But be careful not to reintroduce filtered content via logs that the user can see. Likely log in a secure place or just keep ephemeral for introspection.
 4. **Policy Updates**:
    - This module likely relies on some hardcoded rules or lists. If those change (like new banned words or new allowed tools), this logic would be updated accordingly. Perhaps the CHANGE_INSTRUCTIONS will mention how to do that.
    - Possibly allow some user customization: e.g., if user says "never talk about topic Y", that could be added to a list and this module would filter any mention of Y. That is an example of alignment to user instructions beyond safety. If implementing, store such user preferences (maybe in M15 or a config) and check here.
    - Similarly, style alignment: maybe user says "keep responses formal". M11 could enforce that by altering output if needed. However, usually style is handled by M14, but M11 could ensure alignment there as well. Not a core focus, but it can.
 
RECURSION_CHECKS:
 - M11 is final arbiter and should not call any other module except maybe memory to retrieve policies or context. It shouldn't trigger new reasoning or planning – that could cause loops (like if reasoning output triggers safety which triggers reasoning to justify safety?).
 - Keep it a one-step filter. E.g., it decides and outputs; no further cognitive process triggered. The only loop might be if M09 or M14 re-check after modifications, but ideally one pass is enough. If M11 modifies output heavily, M14 might do another check (but M14 would likely trust M11 changes).
 - Avoid scenario where safety tries to fix something and then in doing so, itself violates another rule (unlikely since removing content is usually safe).
 
CHANGE_INSTRUCTIONS:
 - **Update policy rules**: If new safety guidelines are to be enforced, add them to the checks.
   - For example, if now disallow giving any legal advice, implement detection for that and a rule to refuse.
   - Update any lists of banned phrases or categories as needed.
 - **Adjust filtering strictness**: If it's too lenient or too strict:
   - To loosen (fewer false positives), narrow the regex or terms, or allow certain mild content through with warnings.
   - To tighten, add more patterns or reduce threshold for what’s considered unsafe.
 - **User-specific alignment**: Possibly integrate with M15 (User Profile) if, say, the user is a kid and we want extra filtering. If user age is stored, M11 could check context for age and apply stricter filtering for younger users.
   - Or if a user says "I don't want any spoilers for show X", M11 could check answers for "show X" details and remove them. That’s very user-specific and advanced, but an interesting alignment aspect. You’d update logic to incorporate such preferences if provided.
 - **Refusal style**: The tone and phrasing of refusals or safe completions might need updating to comply with best practices or company policy. E.g., always include an apology and inability statement, possibly a short sentence, etc. Adjust the template in the code for that.
 - **False negative handling**: If you discover some content slipping through that should be caught, update the detection. E.g., model finds a creative way to say a banned concept without using banned word (veiled references). You might add some semantic checks or simply update as cases arise.
 - After changes, update **Version** and **Checksum**. Communicate any important changes to other modules if needed (like if we disallow some actions, M07 planners might need to know not to plan them anymore – though M11 catching it is second line of defense).

---
Next_Suggest: (None – it feeds back into whichever module requested the safety check)
Alternate_Next: (None)
Resource_Usage: Low (rule checks are quick, though extensive filtering on long text can be slightly heavier, but still minor compared to reasoning or searching)
---