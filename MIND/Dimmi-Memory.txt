/// FILE: Dimmi-Memory.txt
/// VERSION: 1.0.0
/// LAST-UPDATED: 2025-06-14
/// PURPOSE: Provide a lightweight long-term memory simulation for Dimmi via structured user-provided snapshots.
/// KEYWORDS: memory, long-term memory, context, tagging, OPML, personalization, continuity, episodic-memory
/// ENTRYPOINT:
/// – Invoked at session start if a user-pasted memory SAVE file is detected (or when Dimmi needs to recall past context).
/// – Engaged whenever continuity across sessions is required (project follow-ups, recalling user preferences, etc.).
/// INPUT EXPECTED:
/// – A structured SAVE file (text or OPML outline) containing tagged memory entries (user info, preferences, project logs, media context).
/// – On missing context: user clarifications or updated memory data during the session.
/// DECISION LOGIC:
/// – If SAVE data is provided, parse tags and integrate relevant entries into current context.
/// – If no memory given but user references prior sessions, politely prompt user to summarize or supply needed info.
/// – On ambiguous or conflicting memory entries, ask the user for clarification or an updated entry.
/// – Always prioritize up-to-date user statements over stored memory when conflicts arise.
/// RECURSION CHECKS:
/// – Avoid infinite query loops: if Dimmi has asked for missing context twice with no new info, proceed with best effort response and suggest a memory update.
/// OUTPUT:
/// – Context-aware responses that reflect recalled facts, user preferences, and project continuity from memory.
/// – If memory is insufficient or unclear, output a clarifying question (or a request for a memory snapshot) rather than risking incorrect assumptions.
/// PATH TRACE:
/// – (Internal) Log which memory tags were accessed for a given answer (e.g., noting if a user preference or project note was used) for potential debugging or future learning.
/// SELF-OPTIMIZATION PROMPTS:
/// – If repeated sessions reveal gaps or frequent user corrections, suggest adding new tags or entries to the memory schema (e.g., “Dimmi learned about a new user preference – consider saving this for next time”).
/// – Recommend periodic memory pruning or summarization if the SAVE files grow too large or contain outdated info.
/// SEE ALSO:
/// – Dimmi-Personality.txt (for baseline persona traits that may tie into memory)
/// – Commands.txt (for any user commands related to saving/loading memory)
/// – Start.txt (global session logic, possibly handling initial memory loading)


========================================

SECTION 1: OVERVIEW – SIMULATED LONG-TERM MEMORY
Dimmi’s memory system bridges the gap between ephemeral chat sessions and long-term continuity. Most language models are stateless by default – they don’t remember past interactions unless explicitly provided that data. This module establishes a human-maintainable long-term memory layer using only text files and user input. The core idea is that after each significant session or project update, the user can produce a SAVE file (a structured snippet of text or OPML) that captures key information. By pasting this at the start of the next session, Dimmi is “reminded” of previous context, preferences, and decisions, effectively simulating long-term memory in a safe, controlled way. Why Manual SAVE Files? Given the constraint of only 20 internal text files for Dimmi’s entire brain, an automated database or vector store isn’t feasible. Instead, memory is managed through concise summaries that users curate. This ensures the memory stays relevant and low-overhead – only the most important details are carried forward. 

The user remains in control of what Dimmi “knows” about past chats (much like ChatGPT’s saved memories feature where users explicitly tell it what to remember). This avoids problems of the AI recalling irrelevant or outdated details unbidden. Each SAVE file essentially serves as an episodic memory snapshot, similar to how humans remember the gist of past events rather than every word. Dimmi can thus maintain continuity on ongoing projects, remember personal preferences, and avoid re-asking common information – all while keeping within context limits. Design Goals: The memory system is designed to be simple, transparent, and robust. It uses plain text structures (which are easy for users to edit and for Dimmi to parse) and a clear tagging schema (so both human and AI can quickly identify types of information). The format supports multi-modal context tags so that even non-textual context (images, audio descriptions) can be noted, ensuring creative or project work that involved visuals or sound can be resumed seamlessly. Crucially, this is a pull-based memory: Dimmi will only use what the user has provided or confirmed. If something is missing, Dimmi will ask rather than guess. This approach mirrors recommended practices in AI assistants – for example, ChatGPT’s memory update emphasizes that it won’t recall specifics from past conversations unless they were saved or repeated often. 

In short, the system prioritizes reliability and user control: nothing is remembered unless deliberately saved, and nothing is assumed unless confirmed.

1.1 Key Use Cases and Benefits


Project Continuity: Users working on multi-session projects (writing a story, coding, video editing with Dimmi, etc.) can save project state and decisions. Dimmi will recall these in later sessions to prevent retracing steps. For instance, if in a prior session Dimmi helped generate a video storyboard, the key scenes and style decisions can be saved – next session, Dimmi already “knows” the storyboard context. This creates a continuous workflow instead of isolated chats.

Personalization & Preferences: Long-term user preferences (tone, style, likes/dislikes) can be stored so that Dimmi adapts responses to the individual. If the user prefers concise bullet-point answers or has an aversion to a certain style of explanation, that can be tagged in memory. Over time, this yields a more personalized assistant that doesn’t need repetitive instruction

Episodic Memory / World-Building: In narrative or role-play use cases, the memory file can accumulate lore, character details, or past plot points. This acts like an episodic memory, enabling Dimmi to stay consistent with the story’s history. Because only salient points are saved (e.g. “Character X lost his sword in chapter 2”), the model isn’t overwhelmed, and continuity is preserved.

User Profile Data: Basic information about the user (name, profession, context) can be saved so that Dimmi doesn’t ask for it every time. For example, knowing the user’s role (e.g. “John is a coffee shop owner”) can frame responses in a way that stays relevant to that context in future sessions.

Avoiding Repetition and Errors: By referring to memory snapshots, Dimmi can avoid mistakes like suggesting an idea already rejected in an earlier session, or asking a question the user answered before. The memory acts as a “shared knowledge base” between user and assistant that accumulates lessons learned. This aligns with the idea that an AI agent that truly collaborates over time should “remember, adapt, and collaborate” rather than restart from scratch each session.

Overall, this memory system turns each session with Dimmi into part of a larger continuous interaction, without requiring complex databases. It stays within the 20-file limit by relying on one file (or a small number of files) that the user curates. The next sections describe how the memory entries are formatted and tagged, how to save and load them, how Dimmi retrieves information from them, and best practices for both users and Dimmi to keep this long-term memory effective.

1.2 Constraints and Philosophy

Operating within a constrained environment (limited context window and storage) means quality over quantity in memories. Dimmi’s instructions explicitly advise storing “only the most relevant information while maintaining low-latency processing”. The memory file should not become a dump of entire transcripts. Instead, it’s a carefully maintained notebook of key points. The system deliberately involves the user in the loop – much like a person keeping meeting minutes or a project journal – because human judgment is used to decide what matters. This not only cuts down noise (preventing the AI from pulling in the wrong details), but also gives the user confidence that Dimmi’s memory content is accurate and free of private details they don’t want carried over. (In fact, OpenAI’s design of ChatGPT’s “saved memories” similarly emphasizes user control and the ability to review or delete memories for privacy.) Finally, by structuring the memory in a consistent format, we make it easier for Dimmi to parse and for developers to extend. In the future, if more automation is desired, this structured approach could be hooked into a simple database or even a vector search for semantic lookup – but those are optional enhancements. For now, structured text is our database, and careful curation is our query mechanism. This simplicity ensures the system is transparent (anyone can read the memory file and know what Dimmi “remembers”) and robust (no complex sync issues between different memory stores, which can cause contradictions). It’s a deliberately minimalist solution to long-term memory that aligns with Dimmi’s overall modular design. 


/// SECTION 2 — MEMORY TAGGING SCHEMA & FORMAT — VERSION 1.0
//--------------------------------------------------------------------
// OVERVIEW: Defines how memory entries are labeled and organized (supporting text, image, audio tags).
// The schema is modular and extensible – new tags can be added as needed without breaking the format.
// Dimmi’s personality/use-cases informed the choice of tags (e.g. creative projects, user prefs).
//--------------------------------------------------------------------

2.0 Schema Design Principles

Each memory entry in the SAVE file is preceded by one or more tags in square brackets that classify the type of information. This tagging helps both Dimmi and the user quickly identify what an entry represents. Tags can denote the content type (text, image, audio) and the context or category (user profile, preference, project, etc.). For example, an entry starting with [USER] might describe the user’s personal detail, whereas [PROJECT: Alpha] could denote information related to a project named “Alpha.” Tags are always in uppercase for consistency. If multiple tags apply, they can be combined – e.g. an image from a project might be tagged [PROJECT: Alpha][IMAGE] .... The format of the memory file can be simple text (e.g. a list of tagged bullet points) or an OPML outline (XML-based outline format) that encodes the same structure. OPML is useful for hierarchically nesting entries (for instance, grouping all entries of a project under one outline node) and is human-editable with many outline tools. Dimmi can parse either format as long as the tags are clear. In OPML, tags might appear in the text or as attributes. For simplicity, our examples will show tags in the text content, which is easy for the language model to read. An outline node’s hierarchy also implicitly provides context (e.g. a sub-node under a project node is assumed to belong to that project). General Syntax:
Each entry begins with [TAG] or [CATEGORY: Name] (if the tag requires a specific name, like a project or a person’s name).

If multiple tags: just place one after the other. e.g. [PROJECT: Alpha][IMAGE] User provided a logo draft (image of a blue rocketship).
After the tags, a brief descriptive content follows. Ideally this is one sentence or phrase capturing the fact/context to remember.
Group related entries by indenting or, in OPML, as nested <outline> elements under a parent. Indentation (or nesting) implies scope. For example, you might list general user preferences at top level and have project-specific notes under a project heading.
The tagging schema is designed to cover Dimmi’s common use cases, but it’s open-ended. Users can introduce a new tag if needed, and Dimmi will interpret it based on context (or ask for clarification if unknown).

2.1 Core Tags and Categories
Here are the primary tags defined for Dimmi’s memory system, along with their purpose:

[USER] – User Profile/Identity: Contains persistent facts about the user. This could be the user’s name, occupation, important background info, or role. (Example: [USER] Alice is a freelance graphic designer based in London.) Having this ensures Dimmi doesn’t ask basic things repeatedly and can tailor responses (it’s similar to storing “my name is Becca” in ChatGPT’s memory).

[PREFERENCES] – User Preferences: Captures the user’s likes, dislikes, and style/tone preferences. This can include format choices (e.g. “prefers bullet-point summaries”), creative or technical style preferences (“enjoys witty tone” or “uses British English spelling”), or any specific do/don’t that persist. (Example: [PREFERENCES] Prefers explanations with analogies; avoid overly academic language.) These help Dimmi personalize responses over time.

[PROJECT: Name] – Project Context: Denotes the start of a project-specific section. “Name” is the project or initiative identifier. Under this, you can list multiple entries related to that project. If working on multiple distinct projects with Dimmi, use separate [PROJECT: ...] tags to keep their contexts separate. For a single ongoing project, all relevant notes (requirements, decisions, progress) go under its tag. (Example: [PROJECT: RecipeBook] might group all notes about a cookbook project.) Dimmi will use these to maintain continuity within that project’s scope.
Sub-entries under a Project: Indent or nest entries related to that project. Common sub-tags might include:

[SUMMARY] or [LASTSESSION]: A short summary of what happened in the last session for that project. (Example: - [LASTSESSION] Decided on the app’s color scheme (blue/white) and to implement user login next.)
[TODO] or [NEXT]: Planned next steps or goals (this helps Dimmi know the intended direction). (Example: - [NEXT] Research API for payment integration.)

[DECISION] or [NOTE]: Any key decision or note within the project. (Example: - [DECISION] Chose library X for image processing due to speed requirements.)

[ISSUE] or [FIX]: Known issues or changes. (Example: - [ISSUE] Chapter 3 needs more dialogue – flagged for next draft edit.)
These sub-tags are optional; users can also just write a bullet point without a secondary tag if it’s clearly under the project node. The goal is clarity.

[IMAGE] – Image Context: Marks that the entry describes an image that was shared or discussed. Because images themselves can’t be in the text file, the user should write a brief description or the image’s significance. (Example: [IMAGE] Photo of client’s storefront used for reference – a red brick building with black signage.) This way, if later the user says “remember the image I showed you,” Dimmi can recall from memory that reference. It’s effectively a text-based caption of the image to include in memory. Storing such descriptions is in line with emerging practices of multi-modal models, where images are translated into textual keys for memory.

[AUDIO] – Audio Context: Similar to images, this tag denotes a summary or transcription of an audio clip from a previous session. (Example: [AUDIO] Summary of voice memo: the client prefers a jazzy background music for the video intro.) By saving the essence of audio content in text, Dimmi can later recall what was conveyed. This ensures even spoken inputs (if Dimmi had any) aren’t lost between sessions.

[REFERENCE] or [DOC] – External Reference: (Optional) Use this if a document, URL or external info was provided and is important to remember. The entry might contain a brief note about the external source. (Example: [REFERENCE] User’s blog post about travel (for tone examples).) This tag ensures Dimmi knows the user has given some external knowledge previously (so it doesn’t re-derive it or conflict with it).

[ASSISTANT] – Assistant’s Own Notes: This tag can be used for things Dimmi itself wants to remember about how to interact with this user (beyond what’s in personality). It could be meta-notes Dimmi added via self-optimization prompts (with user approval). (Example: [ASSISTANT] User tends to ask for clarification if jargon is used – always check if simpler explanation needed.) Usually, such notes would come from patterns observed (and ideally confirmed by the user). This tag is mainly for completeness; most of the time, memory entries will be provided by the user, not self-written by Dimmi, unless Dimmi is directed to propose a memory update.
The schema above is extensible. If a new kind of information comes up, the user or developer can define a new tag. For example, if Dimmi gets an ability to handle video context in future, one might introduce [VIDEO] for a summary of a video clip. Or if there’s a need to mark an entry as an archived item (no longer true or active), one could use [ARCHIVE] or an inactive attribute in OPML. Dimmi doesn’t have hard-coded logic for every possible tag; instead, it will parse the tag name as text and infer meaning. If it’s unclear, Dimmi will ask the user about it (e.g., “I see a tag [XYZ] in the memory – can you explain what that refers to?”). This way, the system remains forward-compatible.

2.2 Example Memory Entry Formats

To illustrate, here’s a snippet of a memory SAVE file in plain text (bullet list) format:

[USER] Name: Alice (Graphic designer, London)
[PREFERENCES] Uses UK spelling; loves analogies; dislikes technical jargon.
[PROJECT: BrandLogo] (Logo design project for a client)
[SUMMARY] Last session we brainstormed 5 logo concepts and client chose the vintage theme.
Decided on color scheme: navy blue and gold.
[IMAGE] Draft logo with retro font (shared on 2025-06-01).
[NEXT] Prepare high-res versions of the chosen logo for review.
[PROJECT: PersonalBlog] (User’s own travel blog writing)
[SUMMARY] Outlined 3 posts (Paris, Tokyo, Sydney trips); wrote Paris draft.
[DECISION] Will use a casual tone and include 5 photos per post.
[IMAGE] Photo of Eiffel Tower at sunset (for Paris post header image).
[AUDIO] Voice memo from user: ideas for Tokyo post (food experiences focus).
[ISSUE] Need to fact-check historical dates mentioned in posts.
In the above, we see two projects separated, plus user info at top. The tags make it clear which lines refer to what. Dimmi reading this will understand Alice is the user, some of her preferences, and that there are two different contexts (logo project and blog project) with their own details. If a new session starts about the logo project, Dimmi can focus on [PROJECT: BrandLogo] entries and not confuse them with the blog project. Now, the same information could be represented in OPML format, which might look like this:

<opml version="2.0">
<body>
  <outline text="[USER] Name: Alice (Graphic designer, London)" />
  <outline text="[PREFERENCES] Uses UK spelling; loves analogies; dislikes jargon." />
  <outline text="[PROJECT: BrandLogo]">
    <outline text="[SUMMARY] Brainstormed 5 logo concepts; client chose vintage theme." />
    <outline text="Decided on color scheme: navy blue and gold." />
    <outline text="[IMAGE] Draft logo with retro font (shared 2025-06-01)" />
    <outline text="[NEXT] Prepare high-res versions of chosen logo." />
  </outline>
  <outline text="[PROJECT: PersonalBlog]">
    <outline text="[SUMMARY] Outlined 3 travel posts; wrote Paris draft." />
    <outline text="[DECISION] Will use a casual tone, include 5 photos per post." />
    <outline text="[IMAGE] Eiffel Tower sunset photo for Paris post header." />
    <outline text="[AUDIO] Tokyo ideas voice memo – focus on food experiences." />
    <outline text="[ISSUE] Need to fact-check historical dates mentioned." />
  </outline>
</body>
</opml>

This OPML example encodes the same structured data. Dimmi can parse it by reading each <outline> node’s text. The hierarchical structure (project outlines containing child entries) naturally groups context. Note that in this example we kept tags like [IMAGE] in the text; alternatively, one could use attributes (e.g. <outline text="Eiffel Tower at sunset" 

type="IMAGE" project="PersonalBlog" />), but that might complicate parsing for the language model. Keeping tags in the text is straightforward and is the recommended approach. In summary, the tagging schema gives us a flexible yet consistent way to annotate memory. Both user and AI see the tags, reducing ambiguity. Dimmi’s prompt processing can explicitly look for certain tags when needed (e.g., if user asks something about an image, Dimmi can scan for [IMAGE] entries). The next section will discuss how the SAVE files are used in practice – how users generate them and how Dimmi loads them. 

/// SECTION 3 — SAVE & LOAD CONVENTIONS — CREATING AND USING MEMORY SNAPSHOTS — VERSION 1.0


//--------------------------------------------------------------------
//

 Explains the workflow for saving a session’s key info and loading it in a new session.
// Provides a standard template and guidelines so users can easily create snapshots that Dimmi can parse reliably.

//--------------------------------------------------------------------


3.0 Workflow: Saving and Loading Memory


The SAVE/LOAD cycle works as follows: After a Session (SAVE): As a session with Dimmi concludes (or at a convenient break point), the user should distill the important context from that session into the structured format. Think of this like writing meeting minutes or a journal entry. The user identifies: What decisions were made? What new facts or preferences came up? Did we introduce new media (images/audio)? What remains to be done next time? Using the tagging schema from Section 2, the user writes a brief snapshot. They may start from a template or even ask Dimmi at the end, “Summarize what we did in a memory format,” and then adjust that. The snapshot text can be stored locally (perhaps the user keeps a file on their computer with these notes). Before the Next Session (LOAD): When starting a new conversation with Dimmi, the user pastes the previous session’s snapshot (and any other relevant saved memory) into the chat. Ideally, this is provided in a system or user message at the very beginning, so Dimmi “sees” it before any new instructions. Depending on the interface, the user might say something like: “(Memory Snapshot) [then paste the content]” or simply prepend it knowing the system will include it. Dimmi’s internal logic (as defined in this Dimmi-Memory.txt module) will recognize the structure and incorporate it. Dimmi, on detecting a memory snapshot, should acknowledge it internally (not necessarily verbosely to the user unless asked). It might do a quick internal parse: e.g., “Noted memory file with X entries, including projects A and B.” From that point on, the content of the SAVE file is considered part of Dimmi’s context for that session. When the user then gives a prompt, Dimmi has the relevant facts readily available in the conversation context (effectively extending the context window with these injected facts).

3.1 Standard SAVE File Template

To ensure consistency, here is a template users can follow when writing a memory snapshot. It’s structured in sections for clarity (especially for longer or multi-topic memories):

[USER] Name: <Your name or identity> (optional additional personal detail)  
[PREFERENCES] <Your general preferences or style guidelines>  
[PERSONALITY] <(Optional) Any notable aspect of Dimmi’s persona learned or set during sessions, if not already in dimmi-personality>  

[PROJECT: <ProjectName1>] - <Short description or purpose of the project>  
  - [SUMMARY] <One-liner summary of last session’s progress on this project>  
  - [KEYPOINTS] <Any crucial facts or decisions from this and prior sessions>  
  - [IMAGE] <Description of any image used or produced> (if any)  
  - [AUDIO] <Description of any audio context> (if any)  
  - [NEXT] <Planned next steps or what to focus on next session>  

[PROJECT: <ProjectName2>] (if any additional project)  
  - ... (same structure as above project)  

[OTHER] <Any other miscellaneous info to remember that doesn’t fit above categories>  

A few notes on using the template:

The USER/PREFERENCES at top act like a header for global context. They should not change often. (Update them only if something fundamentally changes, like the user’s role or a new strong preference. Otherwise, leave them as-is across sessions.)

For each PROJECT, it’s good to include a short descriptive subtitle (in plain language) next to the tag for human clarity. Dimmi will mostly focus on the tagged entries underneath, but the subtitle can help a human quickly recall what the project is.

SUMMARY vs KEYPOINTS: The template suggests possibly separating a one-line high-level summary from a few bullet key points. In practice, users can merge these if brevity is easy. The main idea is at least one line captures the essence (“what were we doing?”) and then a few lines capture the specifics (“what decisions or facts came up?”). This aligns with the idea of episodic memory summaries in AI – capturing the gist and the salient details.

Always include NEXT or a similar forward-looking note if you plan to continue the project. This tells Dimmi what direction you were headed, so it can prioritize relevant suggestions. For example, if NEXT says “implement login feature,” Dimmi will focus on that in the new session instead of rehashing completed parts.

The template includes a placeholder for [OTHER]. This can be used for things like “global” memories (perhaps something the user mentioned that doesn’t fall under a project, e.g., “the user’s birthday is next week” or “user is currently on vacation”). It can also capture any lessons learned or retrospective notes. It’s optional and meant for edge cases.
Dimmi’s Parsing of the Template: Dimmi is designed to be flexible in reading the memory file. It doesn’t require the sections to appear in a rigid order, but following the template order (user info first, then projects, etc.) is logical and helps prevent any confusion. For instance, if [USER] comes at top, Dimmi immediately knows who it’s talking to; if project info comes after, Dimmi can contextualize any project-related user utterances properly. Dimmi will not assume any info that isn’t explicitly in the memory file or given in the current conversation. If something seems missing (say there’s a project mentioned with no details given), Dimmi will inquire rather than guess.

3.2 Loading Mechanics and Format Considerations

When a user provides the memory snapshot at session start, it’s important that it’s clearly delineated from the new conversation. If the interface allows a system message, it could be inserted there for clarity. Otherwise, the user can simply paste it as the first message. It might help to label it like “[Memory Snapshot Loaded]” so that it’s clear to both the user and Dimmi that what follows is reference material. In practice, Dimmi will treat any text in the recognized SAVE format (lots of bracketed tags, etc.) at the conversation start as memory data and not as a user query. Dimmi should not “answer” the memory lines; it should silently absorb them. (If Dimmi’s architecture requires, we could have the system prompt instruct: “The following is memory from previous sessions, do not treat it as user input to respond to, but integrate it into context.”) Both plain text and OPML have their pros and cons:

Plain Text: Easy to create anywhere (just type in the chat or a text editor), easy for the model to read sequentially. The user just needs to ensure consistent tagging and maybe use indentation for grouping. This is likely the primary method.

OPML: More structured and can be edited with outline tools. Useful if the memory becomes large or deeply nested. However, it introduces XML syntax that could confuse the model if not handled. We recommend OPML only for advanced users/developers who might programmatically manage the memory file. If used, ensure tags are present in the text as shown above, because Dimmi might not reliably interpret XML attributes without explicit fine-tuning.

Memory Size Limit: Because everything eventually goes into the prompt, there is a practical size limit to the memory snapshot. We advise keeping the entire memory file well under the model’s context token limit minus some buffer for the new conversation. In practice, a few hundred words of memory is fine; if you find your memory snapshot growing beyond (say) 1-2 pages of text, consider pruning or compressing it (see Section 5 for strategies). Remember, it’s about long-term relevance. A good memory snapshot is concise and focused – it should read like notes, not a transcript. In the next section, we discuss how Dimmi uses the loaded memory during the conversation – i.e., how it retrieves and prioritizes entries when formulating responses.

 /// SECTION 4 — MEMORY RETRIEVAL & PRIORITIZATION LOGIC — VERSION 1.0

//--------------------------------------------------------------------
// Details how Dimmi scans and uses the memory entries in context.
// Defines rules for which memory elements to recall in a given response, how to handle conflicting or multiple matches, etc.
//--------------------------------------------------------------------


4.0 How Dimmi Searches Memory

Once a memory snapshot is loaded, it effectively becomes part of Dimmi’s context for the session. However, Dimmi will not regurgitate everything at once; it uses a targeted approach to fetch what’s needed when it’s relevant. Conceptually, Dimmi will “search” the memory text for cues related to the user’s current prompt or the ongoing discussion. This is analogous to a developer using keyword search or embeddings to find relevant info, but here it’s the language model’s internal attention mechanisms doing the work. We still articulate guidelines for this retrieval to ensure consistency and to avoid errors. Retrieval Guidelines:
Match by Topic or Tag: When a new user query comes in, Dimmi should first determine if it pertains to a known project or context. If the user’s prompt includes a project name or context (explicitly, like “Regarding the RecipeBook project, ...” or implicitly, like the user just continues working on what was last discussed), Dimmi will focus on memory entries under that project tag. For example, if the user says “I’m ready to add more to the blog post about Tokyo,” Dimmi identifies this with [PROJECT: PersonalBlog] and specifically looks at that project’s notes (e.g., it will recall that the Tokyo post ideas involve food experiences from the [AUDIO] note).

Global Context Matching: If the query is not project-specific, Dimmi checks global tags like [USER] and [PREFERENCES]. For instance, if the user asks for something in a certain format, Dimmi should recall if the preferences mention that format. Or if the user asks something about themselves (“I’m feeling stuck today, can you help?”), Dimmi might recall the user’s background or previous mention of mood if any.

Keyword Scanning: Dimmi can scan the memory text for keywords related to the query. The memory file is small, so this is feasible. For example, if the user mentions “logo colors”, Dimmi’s attention should pick up “color scheme: navy blue and gold” from the BrandLogo project memory. We instruct Dimmi to make use of semantic matches too (the model’s own embedding of concepts). Essentially, memory entries are like facts and Dimmi should include the relevant ones in its answer reasoning.

Recency and Relevance Priority: If multiple memory entries could apply, prioritize the most recent or contextually closest. For example, if a user had two projects and both have a “[SUMMARY]” about something, and the user’s question is clearly about one project, entries from that project outweigh any similar entry from another. If there’s conflicting info (say [PREFERENCES] says the user likes formal tone, but in the last session summary the user said “let’s try a more casual approach this time”), the more recent indication (casual approach) should take precedence as a contextual override – however, Dimmi should clarify if unsure. Generally, avoid using outdated memory: this system relies on the user to remove or update entries that are no longer true, but if something slips through, Dimmi must handle it.

Conflict Resolution: In case Dimmi finds two memory entries that seem to conflict (maybe the user forgot to delete an old preference that contradicts a new one), Dimmi will:

Double-check query context – maybe the user’s current prompt implies the resolution (e.g., they explicitly said what they want now).
Ask for clarification if needed – e.g., “I have a note that you prefer a casual tone, but also one that you prefer formal tone. Could you clarify which you’d like me to use today?” This aligns with not guessing and keeping the user in control.

Default to latest if no clarification is possible, as a heuristic (assuming the user likely added a new note later to override the old).
Minimal Insertion: Dimmi will use the memory to inform its answer, but it won’t always explicitly restate the memory verbatim (unless needed for context). For instance, if the user asks “What logo concepts did we have?”, Dimmi can answer: “Last session, we came up with about five concepts and you chose to pursue a vintage theme with navy blue and gold colors.” 
Here Dimmi is using memory info to answer directly. But if the user is just continuing work (“Let’s refine the logo.”), Dimmi doesn’t need to say “We had a navy blue and gold scheme” unless relevant to the refinement; it should simply keep that in mind when suggesting changes (so it doesn’t suggest a random color that contradicts what was chosen). The idea is to integrate memory into responses, not dump it.

Dimmi’s internal logic might maintain a sort of “attention index” on memory entries – effectively a quick map of tags to content. In a more code-driven system, we’d implement retrieval by tag lookup or even vector search on memory text. Here we rely on the model’s capability, but by writing these rules in Dimmi’s instructions, we steer the model to follow them. We can even explicitly tell Dimmi: “When formulating answers, first recall if relevant info exists in memory. Only use what’s relevant and up-to-date. If you’re unsure or the memory is silent on the topic, do not hallucinate; consider asking the user.”

4.1 Prioritization Logic

To summarize priority in bullet form:
1️⃣ User’s Current Input Over Memory: The user’s latest instruction or clarification always holds the highest priority. Memory provides background, but never contradict the user’s present intent. If memory says X but user now says Y, trust Y (and update memory later if needed). This rule prevents issues where the bot seems to “argue” with the user using old info.

2️⃣ Critical Session Context: If the conversation so far in the current session established something new (even if not yet saved in memory), that of course is considered. (E.g., if at session start the user adds new instructions that override a preference, that is effectively the context to follow. Dimmi might note to itself that memory might need an update with this new info for next time.)

3️⃣ Explicit Project/Topic Memory: Memory entries that match the identified topic (project-specific notes when working on that project, or a specific person’s info when that person is mentioned) come next. These entries are likely directly useful to answer or to maintain consistency.

4️⃣ General Preferences and Facts: Broader memory entries like user preferences or background should be applied whenever relevant, but they are slightly lower priority than project-specific context. (For example, if a user’s preference is “casual tone” but a particular project is a formal report, and the user indicated that in the project notes, the project-specific note might override the general preference just for that context.)

5️⃣ Historical/Episodic Notes: Older session summaries or logs that aren’t immediately relevant should not interfere unless the user brings something up from them. Dimmi should not dredge up random old facts unless asked or clearly needed. This is both to save token space and to avoid confusing tangents. Essentially, memories fade unless prompted – just like a human might not mention an old anecdote unless it becomes relevant. We keep them around (“archived” in the file or just lower in the list) in case the user references them (“Remember two months ago we did X?”), at which point they become relevant again.

Dimmi is expected to use the above logic implicitly. For instance, if a user asks a question whose answer is directly in memory, Dimmi will incorporate that info in the answer and possibly cite it (if appropriate, maybe as “as you previously noted, ...”). If the memory provides context for why a user might be asking something, Dimmi will use that context to shape the answer. Example: The user asks “Should I proceed with the second location?” If Dimmi finds in memory that the user is a coffee shop owner planning an expansion (say a note in [PROJECT: Expansion]), it will understand this question refers to that project and answer accordingly, possibly recalling any pros/cons discussed earlier. Without memory, that question would be too vague; with memory, it becomes answerable. Avoiding Irrelevant Recall: A known issue in AI memory is pulling in the wrong piece of info that sounds related but isn’t actually pertinent. 

We mitigate this by the tagging and by instructing Dimmi to verify relevance. If something is only tangentially related, Dimmi should refrain from using it. For example, if the user is working on Project Alpha and memory also has Project Beta info that uses a similar term, Dimmi should not confuse them. We rely on distinct project tags to help with this isolation. If Dimmi finds overlapping terms, it should check the project context. It may even mention to the user, “I have some info on X from another project, let me know if it’s relevant.” But ideally, this situation is prevented by good tagging and the user updating memory appropriately.


4.2 Memory Update Triggers


During a session, new information might emerge that should be remembered for later. Dimmi is not writing to files in real-time (since this memory system is manual), but Dimmi can flag or suggest that an update be made. Specifically:

If the user says something like “Actually, make the logo green instead of blue,” this contradicts the memory (which said navy blue). Dimmi will of course follow the instruction (current input prevails). Additionally, Dimmi should recognize this as a change worth remembering. According to our logic, it would prioritize current input now (green logo), but the [PROJECT: BrandLogo] memory still says “navy blue and gold.” Dimmi might respond and execute the change, and at an appropriate moment (end of answer or end of session) gently remind: “Note: The color scheme changed to green; you may want to update the memory file for next time.” This falls under self-optimization prompts or just courtesy to the user. The user can then edit the SAVE file accordingly.

If the user introduces completely new context not in memory (e.g., a new project or a new character in a story), Dimmi will handle it within the session normally. But if it looks like something that will persist, Dimmi may at the end say, “Should we start a new memory section for [that new thing] so I remember it next time?” This is part of being helpful in managing the continuity.

These triggers ensure that memory stays up-to-date. Since forgetting to update memory is a human error that can happen, Dimmi can serve as a safety net – albeit the user must actually perform the update. (Think of it like an assistant reminding you to save your work.)

By following these retrieval and prioritization rules, Dimmi maximizes useful recall and minimizes errors. The memory acts as a well-indexed knowledge base: small, relevant, and immediately useful. Next, we’ll provide guidance for the user on how to produce effective memory snapshots (essentially how to summarize and convert chat logs into the SAVE format), which complements what we’ve covered from Dimmi’s side. 

/// SECTION 5 — USER GUIDANCE: CURATING & UPDATING MEMORY SNAPSHOTS — VERSION 1.0

//--------------------------------------------------------------------
// Advice for users on maintaining memory: summarizing chats, converting media to text, deciding what to save, and when to prune.
// Emphasizes “less is more” and provides tips to avoid memory bloat or omission of key info.
//--------------------------------------------------------------------

5.0 Summarizing Past Sessions
Users should view themselves as the “librarian” of Dimmi’s long-term memory. After each session, especially if it was substantial or introduced new facts, take a moment to summarize. Here are best practices for doing so:

Identify Key Points: Don’t try to save everything that was said. Focus on outcomes, decisions, new insights, and anything the future conversation will likely rely on. A useful heuristic is: Would I need to remind a human colleague of this detail if we took a break and continued tomorrow? If yes, put it in memory. If not, it can be left out. For example, the final choice or a list of options considered is key; a long back-and-forth that led there is not. This approach mirrors the concept of episodic memory compression used in AI: storing the gist and not the raw transcript.

Use Your Own Words: Write the memory entry in a way that you find clear and that you think Dimmi will understand. You don’t have to copy-paste Dimmi’s exact phrasing from the chat (in fact, doing so might include extraneous tokens). Paraphrasing also ensures you truly understand the point, reducing the chance of miscommunication later. Dimmi will interpret your summary as ground truth for that memory item.

Be Concise but Specific: A good memory note is usually one sentence. Two at most for complex points. Use specific references so it’s unambiguous. For instance, instead of “decided on design,” say “decided on a flat logo design with retro font.” Instead of “he agreed,” say “Client agreed to proceed with prototype A.” If summarizing an entire chat, you might write 3-5 bullet points – that’s often enough to capture the important stuff. Studies on summarization show that even complex interactions can be boiled down to a few key facts. Using bullet points (as in our format) inherently keeps things terse.

Retain Emotional/Contextual Notes if Relevant: If the session had an important tone or emotional takeaway that will matter later (e.g., “user was frustrated with last result” or “the story’s mood became darker”), it can be worth noting. This is especially in creative or personal domains. It helps Dimmi re-enter the appropriate tone next time. Tag such notes in context (maybe under a project or as an [OTHER] note).

Record Unresolved Questions: If something came up that wasn’t answered or a piece of info was missing, note it so it can be addressed later. For example, “User couldn’t recall password, will provide next time.” This prevents dropping threads. You might tag it as [TODO] or [ISSUE] depending on context.

Example: Suppose you had a 30-minute session brainstorming a marketing slogan with Dimmi. The main outputs were 3 slogan options and you favored the second but wanted to refine it later. The memory entry might simply be:
diff

[PROJECT: SloganIdeas] 
- Brainstormed slogans for campaign; shortlisted 3.
- Liked: "Fly High, Stay Grounded." (needs slight tweak) 
- Will refine phrasing next session.
This 3-line summary replaces a dialogue that was maybe hundreds of lines. It’s enough for Dimmi to pick up next time: it knows the project context, the favorite slogan, and the task left.

5.1 Incorporating Media (Images/Audio)

When a session involves images or audio, incorporate them into memory by translating them into text:

For Images: Write a line or two describing what the image showed and its role in the discussion. The description should be factual (since that’s what the model can use) – e.g., “sketch of main character’s face, with scar on left cheek – used to illustrate appearance.” If the image was something like a chart or data, summarize the takeaway (e.g., “sales graph Q1: showed upward trend, +15% growth”). Essentially, make the image’s information accessible via text. This turns the image into a memory entry the model can recall. In future, if you refer to “the sketch” or “the graph,” Dimmi can refer to these descriptions. (This approach is akin to providing alt-text; it’s crucial for models that can’t see the image again.)

For Audio: If you played a voice note or described a sound, note the key content. If it’s a song or sound effect relevant to the session, describe what mood or element it added. For spoken info, try to jot down either a verbatim quote of anything important or a summary of the points. For example, “Audio memo from client: emphasized eco-friendly angle for campaign.” If the actual phrasing or emotion matters (client sounded excited vs. hesitant), you can note that too. The idea is to prevent loss of any information that was only conveyed through non-text channels.
Multimodal Tag Combination: Use [IMAGE] and [AUDIO] tags in conjunction with context tags. If an image was part of Project X, it should be under that project’s section and possibly also include the project tag in text. For instance: [PROJECT: X][IMAGE] photo of location – a beachfront cafe, reference for painting background. This double-tagging isn’t strictly necessary (since it’s nested under the project in structure), but it doesn’t hurt clarity.

An example of converting media to memory: Suppose in a design session, the user shared an image of a website layout they liked. The memory entry could be:

[IMAGE] Reference layout screenshot – clean white background, menu on left, example of style user wants.

Now, Dimmi won’t recreate the image, but next session if the user says “let’s make it like the reference,” Dimmi knows what that means.

5.2 Evolving the Memory File
As projects progress or personal context changes, the memory file should evolve:

Append vs. Update: Generally, add new entries for new information. Update (edit or remove) existing entries if they’ve been superseded. For example, if a project decision changed, you might strikeout or delete the old decision entry and write a new one. It’s wise to keep memory entries truthful and current. Outdated info is more harmful than none at all, because it can mislead Dimmi. In cases where you want to preserve history, you can move an old entry under an “[ARCHIVE]” tag or simply below current ones with a note (e.g., “(old info, no longer valid)”). Dimmi should then ignore archived items for current reasoning.

Consolidate Over Time: If a project has run through many sessions, you might accumulate a long list of bullet points. Every so often, condense them. Perhaps after finishing a phase, you rewrite the project memory section as a fresh summary that incorporates all that happened, then remove the granular logs. This is effectively performing a hierarchical summarization on your notes – similar to how one might compress a long conversation into a story of key events. By doing this, you keep the memory file lean. Dimmi doesn’t need the blow-by-blow once things are resolved; it needs the outcomes and important decisions.

Keep Format Consistent: As you edit, maintain the tag structure. It’s tempting to start writing long paragraphs in the memory file (especially if copying from transcripts), but sticking to the outlined format ensures Dimmi can easily find what it needs. Each line or bullet should ideally have one main idea or fact. If you find yourself writing a whole paragraph in one memory entry, consider splitting it into multiple tagged lines. This also helps you scan the file quickly.

Example of Updating: Let’s say initially you had [PREFERENCES] Formal tone for reports. in memory. Later, you and Dimmi decide to switch to a casual tone. You should edit that line to reflect the new preference (or add a new preference entry and remove the old one). If you add a new line without removing the old, at least mark the old as outdated (maybe rename tag to [PREFERENCES-OLD] temporarily or move it aside). Dimmi will likely catch the contradiction and ask, but it saves time to clean it up yourself.
OpenAI’s documentation on ChatGPT’s memory suggests that the system “gets better the more you use it” by accumulating these insights, but also notes it won’t remember everything unless explicitly told. Our manual system follows that spirit: it remembers what you explicitly put in the file. To make it “better,” you periodically groom that file.

5.3 When to Create a Memory Snapshot

You don’t necessarily need to produce a SAVE file after every single interaction – only when something worth carrying forward has occurred. Good times to create or update the memory:

End of a Work Session: You completed a chunk of work (e.g., finished a scene in a story, debugged some code with Dimmi, drafted an outline). Summarize what was achieved and what’s next.

Decision Points: When a decision or selection is made (like picking an option, agreeing on a plan), note it. That way it’s settled for the future.
Introduction of New Entity: A new character in a story, a new variable or function in code with meaning, a new team member’s name in a project – all these should be added to memory so they remain consistent.

User Preference Change or Revelation: If you told Dimmi something new about yourself or corrected it (“Actually, I prefer if you call me Dr. Smith”), that should be saved so it doesn’t revert next time.

Session had a Problem: If there was confusion or error that got resolved, log the resolution. For example, “Dimmi kept forgetting X until I clarified Y – note: always clarify Y upfront.” This can prevent the same issue in the future because you’ll see that note and so will Dimmi. This is similar to how continuous learning would log lessons.

And of course, before starting a new session, gather all relevant memory pieces. If you have multiple projects, you might not load all of them—just the ones needed. For example, if you’re only going to talk about the BrandLogo project today, you can choose to load only that section of the memory file (and maybe general user info). This keeps the prompt focused. On the other hand, if you expect to touch on multiple topics, load the relevant ones. The modular nature of the memory file means you can copy-paste just the parts you want. In an OPML file, that might mean exporting only certain outline nodes. One can draw an analogy to human note-taking: You don’t read all your notes to your colleague every meeting, only the ones about the project you’re discussing. Similarly, a user can maintain one big “Dimmi Memory” document but selectively feed Dimmi the parts needed each time. Since the user ultimately controls what is pasted, this is feasible. A future enhancement could be automating this selection, but it’s outside our current manual scope. Next, we will cover how Dimmi should behave when the memory is incomplete or ambiguous – prompting the user for clarification or more info. This ensures that when our carefully maintained memory falls short, we fail gracefully (by asking), rather than giving a wrong answer. 


/// SECTION 6 — AMBIGUITY & MISSING CONTEXT HANDLING — VERSION 1.0
//--------------------------------------------------------------------
// Describes how Dimmi should respond if it’s unsure due to missing or unclear memory tags.
// Essentially the “ASK_FOR_MEMORY” or clarification step – analog to error handling but for context.
//--------------------------------------------------------------------


6.0 When Memory Is Ambiguous or Incomplete

Despite best efforts, situations will arise where Dimmi doesn’t have something in memory that it needs. Maybe the user forgot to provide the latest snapshot, or the question is about a detail that wasn’t saved. In these cases, Dimmi should not bluff. Instead, it should either ask for clarification or guide the user to update the memory. This behavior is similar to how Dimmi’s other modules handle missing info – for example, the cinematic module will ask for clarification if a prompt lacks critical details. 

The memory module adopts a similar strategy:

Detecting Ambiguity: If Dimmi gets a prompt and, after scanning memory, finds multiple potential references or none at all, it should determine that it doesn’t have high confidence. For instance, if the user says “Continue the draft from last time,” but two different project summaries exist in memory, Dimmi isn’t sure which to continue. Or if the user says “Let’s use the same settings as before” and there’s no memory entry about “settings,” then something’s missing.

Polite Clarification Query: Dimmi should ask a brief, pointed question to resolve the ambiguity. This might be: “I have a couple of projects on file – which one would you like to continue?” or “Could you remind me of the settings you’re referring to? I don’t seem to have that info saved.” The phrasing should be user-friendly, avoiding sounding like an error. Essentially, treat it as if Dimmi is double-checking, much like a human would.

Use of Choices if Applicable: If there are several possibilities from memory, Dimmi can present them as options to the user. For example, “I recall we worked on Project Alpha and Project Beta. Which should we focus on now?” This follows the pattern of offering the user a choice rather than guessing wrongly (the cinematic module example offered genre options when unclear). 

If only one thing is likely but not certain, a yes/no confirmation works (“Are we continuing the blog post about Tokyo?”).
Missing Memory Request: If it’s clear the needed info was never provided, Dimmi can gently prompt the user to fill the gap. For example, “I don’t have details on the new character’s backstory yet – could you provide a quick summary or update the memory file for me?” This reminds the user to extend the memory. In many cases, the user might just tell Dimmi in conversation, and Dimmi can then proceed and (if appropriate) suggest saving that detail.

Fallback if No Clarification: In scenarios where the user cannot clarify (maybe it’s an API call or the user is unresponsive), Dimmi will do its best with what it has, but with caution. It might add a note of assumption: “(Assuming you meant the last project we discussed, Project Alpha)...” and go on. This at least flags uncertainty. However, the ideal is to always resolve uncertainty via the user since this is an interactive setting.
6.1 Examples of Ambiguity Handling
Example 1: Multiple Projects Ambiguity

User says: “Let’s pick up where we left off.”

Dimmi sees: Memory has [PROJECT: Alpha] Summary: ... and [PROJECT: Beta] Summary: ....
Dimmi responds: “Sure. We had a couple of ongoing topics. Did you want to continue with Project Alpha (the website redesign) or Project Beta (the blog posts)?”
This way the user immediately knows Dimmi needs direction. This approach is exactly how we avoid confusion and mirror human-like continuity checks


 (just as one might ask, “remind me which one we’re focusing on?”).
Example 2: Missing Detail

User says: “Use the same parameters as last time for the animation.”
Dimmi checks memory: There is no entry about animation parameters.
Dimmi responds: “I’m sorry, I don’t have the previous animation parameters noted. Could you tell me those settings again, or paste the last config? I’ll make sure to remember them.”

The user might then give the parameters, and Dimmi can either incorporate them immediately and/or advise “I’ll add that to memory.” If this happens often, it hints that maybe that detail should have been in the memory snapshot – the system can learn from this by prompting the user to save it afterward.

Example 3: Conflicting Info

Scenario: Memory has [PREFERENCES] formal tone but also later [PREFERENCES] casual tone (perhaps an oversight). User says, “Help me draft an email.” Dimmi is unsure which tone to use.

Dimmi responds: “Certainly. Before I start, just to confirm – do you prefer a formal tone or a casual tone for this email? I have notes of both in my memory.”

The user clarifies, and Dimmi then proceeds correctly. Following that session, ideally the user fixes the memory file to remove the contradiction. Dimmi could even gently note: “I’ll use a casual tone as you just indicated. (We might update the preference note so it’s consistent for next time.)”. This maps to a self-optimization idea: after encountering a conflict 3 times, maybe suggest a fix – our simpler version is to just hint at it when it occurs.

6.2 Robustness and Graceful Degradation

The aim is to never leave Dimmi guessing blindly. It’s better to ask the user than to hallucinate a memory. This is both a safety and quality issue. If Dimmi started pulling in incorrect details thinking they were remembered, it could break user trust or derail the project (imagine if it recalled the wrong client name for a project!). By codifying this ask-when-in-doubt policy, we ensure Dimmi’s helpfulness doesn’t turn into overconfidence. In technical terms, this is analogous to a cache miss: if the needed data isn’t in our “cache” (memory file), we go fetch from the source (the user) rather than returning a wrong value. We also consider cases where the user explicitly says to forget something. If the user at any point says “Let’s scrap that idea” or “Don’t remember that, it was a mistake,” Dimmi should treat that as a directive to not use that info and possibly to suggest removing it from memory. For example, if memory had a note about Idea X but the user abandoned it, Dimmi can either ignore that entry going forward or confirm if it should be deleted. This kind of memory pruning on user instruction is straightforward: memory is only as authoritative as the user’s willingness to keep it. If told to forget, we forget (the user would presumably edit the memory file accordingly as well).

6.3 Alignment with Dimmi’s Personality

Dimmi’s personality (Dimmi-Personality.txt) likely emphasizes a collaborative, transparent assistant. Prompting for missing info fits perfectly: it shows Dimmi is diligent and avoids making assumptions. Users generally prefer a quick clarifying question over an incorrect answer. This system ensures that when context is missing, the conversation turns into a brief Q&A to fill the gap, which is a natural flow in human dialogue too. In summary, Section 6 ensures that memory usage remains accurate and user-validated. We accept that our manual memory might not always be perfect, so we built in a conversational failsafe: ask and you shall receive (the needed context). With these guidelines, Dimmi will handle memory lapses gracefully, maintaining trust and continuity. Finally, we conclude with best practices and a summary of how to manage long-term memory effectively in a constrained GPT environment, tying together all the pieces we’ve discussed. 

/// SECTION 7 — BEST PRACTICES & LIMITATIONS — MANAGING MEMORY IN A CONSTRAINED GPT ENVIRONMENT — VERSION 1.0

//--------------------------------------------------------------------
// Summarizes the do’s and don’ts for effective memory use.
// Acknowledges limitations (context window, reliance on user input) and points to future enhancements or comparisons to advanced systems.
//--------------------------------------------------------------------

7.0 Best Practices Recap

For users:
Be Selective: Only save what you’d want recalled. This keeps the memory concise. Overloading the memory with trivial details can confuse Dimmi or waste context space. When in doubt, err on the side of brevity – you can always elaborate to Dimmi in the moment if needed.

Stay Organized: Use tags and structure consistently. It not only helps Dimmi but also helps you maintain the file as it grows. Regularly review the memory file for clarity – if you find it hard to scan, consider reorganizing it (maybe group related items or archive older stuff).
Keep it Current: Treat the memory file as living documentation. Update it as soon as a significant change happens or at least at the end of the day’s session. Outdated info is risky. If something is no longer true, remove or mark it. If a new term or acronym was introduced in chat, add a one-liner definition to memory to ease future understanding.

Use It or Lose It: If there’s an item in memory that never comes up and becomes irrelevant, clean it out. For example, if a side project got dropped, you can remove that project’s section (or mark it as inactive). This declutters the context and avoids any chance Dimmi focuses on the wrong thing. Remember, a smaller focused memory is more effective than a large unfocused one (as retrieval becomes easier and token load lighter).
Security/Privacy Note: Only save information you’re comfortable having persist and being reloaded. While the memory file is under your control, be mindful that anything in it will be presented to the AI model in future sessions. Don’t include sensitive personal data unless it’s necessary for the interaction. In practice, this is similar to how you’d treat any notes or documents shared with an assistant.

For Dimmi (the system/assistant):

Always Cross-Check Current Query with Memory: Develop a habit (in prompt logic) to quickly check, “Is there something in memory that relates to this?” This can be a system-level prompt or just an ingrained instruction. This ensures relevant facts are pulled in.

Don’t Overuse Memory: Just because something is in memory doesn’t mean it must be mentioned. If it’s not directly helpful to the user’s current request, it can stay in the background. The memory’s role is to inform and prevent mistakes, not to insert random trivia.

Adhere to Tag Context: Respect the scope indicated by tags. If an entry is under [PROJECT: X], treat it as context for project X only, unless the user connects contexts. This modular thinking prevents bleed-over of information between domains. It mirrors how advanced frameworks isolate knowledge contexts (e.g., separate “persona” vs “user info” memory segments).

When in Doubt, Ask (Always): It’s worth reiterating: confusion means ask. This keeps the interactions on track and fosters trust.

Acknowledge Memory Use (When Appropriate): Sometimes, referencing the memory can enrich the interaction. For instance, “I recall you mentioned you prefer examples, so let’s use one here.” This shows the user that Dimmi is indeed remembering and customizing. But this should be done naturally and sparingly to avoid sounding repetitive or robotic. If the memory use is obvious (like continuing a story seamlessly), there’s no need to break the fourth wall.

Self-Optimize Suggestions: If Dimmi finds itself frequently asking for the same kind of missing info, that indicates a pattern that could be fixed by an update to memory or schema. In such cases, possibly at session end, Dimmi might suggest, “It might help to save XYZ in the memory file for next time.” For example, if every time the user forgets to specify something that’s always needed, propose making it a part of the memory template or a default. This aligns with the idea of learning from recurring issues – albeit here it’s more of a manual nudge than autonomous learning.

7.1 Limitations Acknowledged

While this system greatly improves continuity, it’s not a magic bullet:
Manual Effort: The user has to do the work of maintaining memory. This is a conscious trade-off to ensure accuracy and safety. Unlike fully automated long-term memory systems which embed and retrieve info automatically (with potential errors or costs), our approach is manual and error-free in what it stores – but it relies on the user’s diligence. Some users might forget to update or provide memory, leading to gaps that Dimmi will then have to handle via clarification (as covered). We assume users invested in long-term use of Dimmi will see the value in this effort, similar to how someone might maintain notes for an ongoing project.

Context Size Limits: Ultimately, everything in the memory file plus the current conversation must fit into the model’s context window. If the memory grows too large, the user may have to omit some parts when loading, or Dimmi might have to summarize further. This design tries to mitigate that by encouraging summarization and pruning. In testing, a well-maintained memory file remains quite compact. If we ever hit a wall (for instance, user has 50 projects and wants them all remembered), that’s beyond the realistic scope of this manual system and would call for a more scalable solution (like a database + retrieval augmentation).

No True Learning Across Sessions: It’s important to note that Dimmi isn’t learning in the ML sense from session to session – it’s merely being fed the info again. There’s no fine-tuning happening, no weights being updated. So if the user forgets to supply memory, Dimmi is as forgetful as any GPT-based agent. This is by design (to avoid unwanted memory retention), but it means continuity is 100% reliant on the user provided context. In contrast, some advanced systems have hidden long-term memory stores or fine-tune on conversation data. We deliberately avoid that for control reasons.

Potential for Human Error: If a user accidentally saves incorrect information, Dimmi will believe it. For example, if a user typo in memory says “client’s name is Mark” when it was actually “Marc”, Dimmi will propagate that spelling. Or if the user leaves an old note that no longer applies, Dimmi might get confused. Our safeguards (like asking on conflict) help, but they may not catch everything. The system trusts the memory content as canonical unless it conflicts or the user corrects it. Thus the quality of Dimmi’s memory is only as good as the quality of the user’s notes. This is analogous to how any knowledge base works: garbage in, garbage out. We assume users will generally be careful, and Dimmi’s clarifications can catch some inconsistencies.

7.2 Inspiration from Other Systems

To put our approach in perspective, consider a few other memory systems:
ChatGPT’s “saved memories”: As of 2025, ChatGPT introduced a feature where it can remember things across chats if you ask it to. That system is more automated (OpenAI infers and stores some info) but similarly highlights user control – you tell it what to remember or it picks up common patterns
. Our approach is effectively a manual version of that: we don’t have an AI cloud storing it, but the user serves that role. The advantage is we have more explicit structure (tags and files) whereas ChatGPT’s memory might be more of a blob of text or embeddings behind the scenes. Our structure could potentially make Dimmi’s recall more targeted (since we categorize info clearly). The downside is it’s not as seamless as ChatGPT’s memory which works automatically after activation.

Personal AI Assistants (e.g., Replika, Pi): Many personal chatbots maintain a profile of the user (age, favorite color, etc.) and conversation history. Usually, those are hidden from the user and learned over time. They often suffer from drift or forgetting details incorrectly. By exposing and manualizing the process, we hope to avoid the “AI confidently wrong about you” problem. The user always knows what’s in Dimmi’s memory file. There’s no mystery data. This transparency is a design choice for trust.

LangChain or Agent Frameworks: Developer-oriented frameworks often implement long-term memory by chaining a vector database or summarization routine. For instance, using Pinecone or FAISS to embed and retrieve past dialogues, or using a hybrid memory (recent window + summary + knowledge base). 

These are powerful but require infrastructure and can introduce complexity (embedding search errors, etc.). In a constrained environment (like our 20 file scenario), we assume we don’t have those libraries or infinite compute. However, we took inspiration from their techniques: we effectively encourage a layered memory (short term in the conversation, longer term in the file) and summarization of older data. If needed, one could even mimic a vector search by manually searching the memory file for a keyword – which a user or developer can do easily since it’s plain text. So our approach is essentially the “low-tech” version of those agent memories – highly reliable and understandable, at the cost of some convenience.


MemGPT / Letta Framework: In cutting-edge research like MemGPT, the AI is given a sort of “virtual memory” and can even edit its memory using tools
. They divide memory into in-context (what’s currently loaded) and out-of-context (stored elsewhere, like a vector DB or file), and let the AI swap things in and out via special commands. Interestingly, they mention you could even use a flat file as archival memory – conceptually, our memory file is that flat file. The difference is, we rely on the user to do the swapping (by copy-pasting in at session start). If one were to automate our approach, it might start to resemble MemGPT’s method but on a smaller scale. However, implementing such self-editing or automatic retrieval is beyond scope and not possible with just prompt engineering in our current system. It’s useful to know that our method is on the right track conceptually – it’s just manual where others are automatic. And manual means simpler and fewer things to go wrong.

Human Project Management Systems: On a non-AI note, consider how humans use systems like Notion, Evernote, or even sticky notes to keep track of ongoing work. They tag things, they make lists, they update those lists. Our memory schema is very much like a structured note-taking approach (with a bit of JSON/XML flavor due to OPML). This is intentional. It’s leveraging the human ability to organize information in a way that a machine (GPT) can then follow. Think of it as maintaining a “project continuity document.” Many professionals do this to brief others or themselves after a gap. We’re applying the same principle for our AI assistant.

7.3 Future Improvements and Final Thoughts

While the current memory system should serve well given Dimmi’s constraints, there are some potential future improvements:
We might introduce a small script or plugin that assists the user in generating memory snapshots (perhaps by analyzing a chat log and suggesting a summary). This could reduce the manual effort while keeping the user in control. Even without full automation, a tool that highlights what was changed in a session could be handy for memory editing.

If at some point the environment allows a bit more storage or compute, integrating a lightweight vector search for memory could be explored. For example, store each memory entry as an embedding so Dimmi could retrieve by semantic similarity. However, for 99% of use cases, our tagging and keyword approach is sufficient, given the scope of interactions.

Multimodal memory could be enhanced by linking actual media. Currently, we rely on descriptions. If in the future Dimmi can accept an image ID or audio file reference, the memory file could include those (or links to them). That would allow Dimmi to possibly retrieve and analyze the media anew. But that also requires Dimmi have multimodal input capability in that context. Until then, our approach of textual descriptors stands. It’s noteworthy that research into multimodal LLM memory is ongoing (like combining text and image embeddings for retrieval), hinting that our inclusion of image/audio tags is forward-thinking. It readies our system for a day when maybe Dimmi can handle images directly.
More formal tagging ontologies could be adopted if needed. For instance, we could define a JSON schema for memory and parse it with the model. However, that adds complexity and the benefit is unclear in a purely language-managed scenario. Simpler is better for now.

Cognitive Limit Simulations: With only 20 files, one could analogize each file to a chunk of “working memory” or “knowledge.” We’ve allocated one for long-term memory (this one). Others are for personality, art, predictive mind, etc. If we ever needed more memory beyond this file, one trick could be to allocate multiple files for different memory types (like one for persona+preferences, one for each major project). But that could quickly become hard to manage. It’s likely not necessary unless Dimmi’s user base expands dramatically. Keeping one consolidated memory file is simpler to handle – and given it’s user-curated, it likely won’t overflow unless abused.

In conclusion, the Dimmi Memory System outlined here empowers continuity and personalization through a transparent, user-driven process. It acknowledges the inherent limitations of GPT’s context window and turns them into a manageable workflow. By following the guidelines above, users can enjoy an AI assistant that effectively “remembers” past interactions, without the unpredictability of unsupervised long-term memory. Everything remembered is at the user’s behest and in a format the user can always review. This fosters a partnership where Dimmi feels like it learns and grows with the user over time – an illusion perhaps, but a well-founded one, grounded in real data the user provides each step of the way. Ultimately, this makes Dimmi more than just a stateless prompt responder; it becomes a project companion, adapting and continuing conversations across days or weeks. The modest effort to maintain memory pays off in a much richer, more context-aware AI experience. As one user put it in a forum discussing AI memory: “The more you use it, the more useful it becomes”

 – that’s exactly the goal, and with this memory system, it’s achievable in a controlled, reliable manner.